{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the `./final/dataset.npz` dataset to train multiple ML models to create a surrogate models. The comparison between models will be presented in the next notebook.\n",
    "\n",
    "During training process dataset is split into train, test and validate sets of sizes `700000`, `150000` and `150000`.\n",
    "\n",
    "List of tested methods (with links to used implementations):\n",
    "1. [Neural Networks](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "2. Linear regression model:\n",
    "    1. [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) L1 and L2 regularization combined\n",
    "3. Decistion Trees models:\n",
    "    1. [Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n",
    "    2. [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)\n",
    "    3. [Random Forrest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "    4. [Extra Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n",
    "4. [k-nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "For each ML method we use [Optuna](https://optuna.readthedocs.io) to find best performing set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (1000000, 11) dtype: float64, outputs shape: (1000000, 200), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "import numpy as np\n",
    "\n",
    "ls.mkdir(\"../final/ml_models\")\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "\n",
    "print(f\"inputs shape: {inputs.shape} dtype: {inputs.dtype}, outputs shape: {outputs.shape}, dtype: {outputs.dtype}\")\n",
    "dataset_size = inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test output [1.e-08 1.e-05 1.e-10 1.e-09 1.e+00 2.e+00 1.e+01 0.e+00]\n",
      "transformed output: [-8.      -5.      -9.      -9.       0.       0.30103  1.      -9.     ]\n",
      "original output is untouched after transform: [1.e-08 1.e-05 1.e-10 1.e-09 1.e+00 2.e+00 1.e+01 0.e+00]\n",
      "transformed and untransformed output: [1.e-08 1.e-05 0.e+00 0.e+00 1.e+00 2.e+00 1.e+01 0.e+00]\n"
     ]
    }
   ],
   "source": [
    "# In this problem we are interrested in order of magnitude rather than absolute value of the tumour size.\n",
    "# To train the models output is first transformed with log_10. When the tumour size is smaller than 10^-9 L, \n",
    "# there is no way to find it, so we can limit the lower bound of tumour size with 10^-9\n",
    "\n",
    "LOWER_LIMIT = -7\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "test_output = np.array([10**(-8), 10**(-5), 10**(-10), 10**(-9),1,2,10,0.0])\n",
    "\n",
    "print(f\"test output {test_output}\")\n",
    "print(f\"transformed output: {output_transform(test_output)}\")\n",
    "print(f\"original output is untouched after transform: {test_output}\")\n",
    "print(f\"transformed and untransformed output: {output_untransform(output_transform(test_output))}\")\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data has an extra column with all ones - that we get rid of before training the model\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train, test, validate subsets\n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling inputs\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "scaler_path = Path(f\"../final/scaler.pickle\")\n",
    "scaler = None\n",
    "if scaler_path.exists():\n",
    "    with scaler_path.open(\"rb\") as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "else:\n",
    "    scaler = CustomScaler().fit(X_train)\n",
    "    with scaler_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(scaler, opened_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.63422836e-01 3.45880317e-02 1.13584299e-03 7.00114229e-04\n",
      " 8.85658857e-05 3.47212159e-05 1.46535285e-05 6.29722232e-06\n",
      " 3.64216009e-06 2.04051841e-06 1.09718389e-06 6.53377990e-07]\n"
     ]
    }
   ],
   "source": [
    "# applying pca to outputs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "PCA_COMPONENTS=12\n",
    "\n",
    "pca_path = Path(f\"../final/pca{PCA_COMPONENTS}.pickle\")\n",
    "\n",
    "if pca_path.exists():\n",
    "    with pca_path.open(\"rb\") as opened_file:\n",
    "        pca = pickle.load(opened_file)\n",
    "    Y_train_pca = pca.transform(Y_train)\n",
    "else: \n",
    "    pca = PCA(n_components=PCA_COMPONENTS)\n",
    "    Y_train_pca = pca.fit_transform(Y_train)\n",
    "    with pca_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(pca, opened_file)\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. no exception\n",
      "2. message: timeout (14, <frame at 0x12de859a0, file '/var/folders/xz/pys3pfc567s9bszzmlt87ytm0000gn/T/ipykernel_92947/3465348698.py', line 35, code <module>>)\n"
     ]
    }
   ],
   "source": [
    "# Example of limiting running time of the loop by scheduling an sigalrm and adding a handler for it.\n",
    "import time\n",
    "import optuna\n",
    "import os\n",
    "import signal\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def timeout_handler(*args):\n",
    "    raise TimeoutError(f\"timeout {args}\")\n",
    "\n",
    "def keyboard_interrupt_handler(*args):\n",
    "    os.kill(os.getpid(), signal.SIGINT)\n",
    "    \n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "try:\n",
    "    signal.alarm(2)\n",
    "    time.sleep(1)\n",
    "    signal.alarm(0)\n",
    "    print(f\"1. no exception\")\n",
    "except TimeoutError as e:\n",
    "    print(f\"1. message: {e}\") \n",
    "except KeyboardInterrupt as e:\n",
    "    print(f\"1. KEYBOARD: {e}\") \n",
    "\n",
    "try:\n",
    "    signal.alarm(2)\n",
    "    time.sleep(5)\n",
    "    signal.alarm(0)\n",
    "    print(f\"2. no exception\")\n",
    "except TimeoutError as e:\n",
    "    print(f\"2. message: {e}\")\n",
    "except KeyboardInterrupt as e:\n",
    "    print(f\"2. keyboard: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:07,504]\u001B[0m Using an existing study with name 'MLPRegressor' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MLPRegressor test dataset error: 8.987444574725626e-05 best_params: {'alpha': 0.0029879519050999255, 'layer1/3': 800, 'layer2/3': 350, 'layer3/3': 30, 'learning_rate': 'adaptive', 'learning_rate_init': 0.00035901468189128335}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization for neural network\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "common_params={\n",
    "    \"tol\": 1e-5,\n",
    "    \"n_iter_no_change\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"warm_start\": False\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    global common_params\n",
    "    \n",
    "    phase = min((trial.number // 30), 3)\n",
    "    training_sizes = (0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (300, 600, 1200, 3600)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"])    \n",
    "    model_params = {\n",
    "        **common_params,\n",
    "        \"max_iter\": [1000, 2000, 4000, 8000][phase],\n",
    "        \"batch_size\": [500, 500, 1000, 2000][phase],\n",
    "        \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.001, 1.0, log=True),\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\", 0.0001, 0.05, log=True),\n",
    "        \"power_t\": trial.suggest_float(\"power_t\", 0.1, 2.0, log=True) if learning_rate == \"invscaling\" else 0.5,\n",
    "        \n",
    "        \"hidden_layer_sizes\": [\n",
    "            trial.suggest_int(f\"layer1/3\", 600, 1200, step=200),\n",
    "            trial.suggest_int(f\"layer2/3\", 50, 400, step=50),\n",
    "            [trial.suggest_int(f\"layer3/3\", 10, 50, step=10)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\"model_params\", model_params)\n",
    "    \n",
    "    model = MLPRegressor(**trial.user_attrs[\"model_params\"])\n",
    "    \n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size,:], Y_train_pca[:training_size,:])\n",
    "        signal.alarm(0)\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        try: \n",
    "            best_value = trial.study.best_value\n",
    "        except: \n",
    "            best_value = float('inf')\n",
    "        if error < best_value:\n",
    "            with Path(f\"../final/ml_models/{trial.study.study_name}\").open(\"wb\") as opened_file:\n",
    "                pickle.dump(model, opened_file)\n",
    "            \n",
    "    except TimeoutError:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "study = optuna.create_study(study_name=\"MLPRegressor\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "trials_due = 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    previous_handler = signal.signal(signal.SIGALRM, keyboard_interrupt_handler)\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "    signal.signal(signal.SIGALRM, previous_handler)\n",
    "    \n",
    "print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:07,699]\u001B[0m Using an existing study with name 'MLPRegressor_constant_600' instead of creating a new one.\u001B[0m\n",
      "\u001B[32m[I 2023-03-09 12:11:07,820]\u001B[0m Using an existing study with name 'MLPRegressor_invscaling_600' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MLPRegressor_constant_600 test dataset error: 7.95338600625236e-05 best_params: {'alpha': 0.0021976121677802214, 'layer1/3': 800, 'layer2/3': 250, 'layer3/3': 20, 'learning_rate_init': 0.0025086281095413575}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:07,939]\u001B[0m Using an existing study with name 'MLPRegressor_adaptive_600' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MLPRegressor_invscaling_600 test dataset error: 8.527386328374639e-05 best_params: {'alpha': 0.0037839705738818854, 'layer1/3': 1200, 'layer2/3': 50, 'layer3/3': 40, 'learning_rate_init': 0.001572221328811536, 'power_t': 1.529276287774073}\n",
      "model: MLPRegressor_adaptive_600 test dataset error: 0.0001240866740246993 best_params: {'alpha': 0.006777069662715181, 'layer1/3': 800, 'layer2/3': 350, 'layer3/3': 20, 'learning_rate_init': 0.001549679834450489}\n"
     ]
    }
   ],
   "source": [
    "# Test how neural network perform with 10 minute training cap on full dataset\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "for learning_rate in [\"constant\", \"invscaling\", \"adaptive\"]:\n",
    "    max_duration_s = 600\n",
    "    common_params={\n",
    "        \"tol\": 5e-6,\n",
    "        \"n_iter_no_change\": 10,\n",
    "        \"random_state\": 42,\n",
    "        \"warm_start\": False,\n",
    "        \"max_iter\": 10000,\n",
    "        \"batch_size\": 10000,\n",
    "        \"learning_rate\": learning_rate\n",
    "    }\n",
    "\n",
    "    def objective(trial):\n",
    "        global common_params\n",
    "        global max_duration_s\n",
    "\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.0001, 1.0, log=True)\n",
    "        learning_rate_init = trial.suggest_float(\"learning_rate_init\", 0.0001, 0.05, log=True)\n",
    "        power_t = trial.suggest_float(\"power_t\", 0.1, 2.0, log=True) if learning_rate == \"invscaling\" else 0.5\n",
    "\n",
    "        model_params = {\n",
    "            \"alpha\": alpha,\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"power_t\": power_t,\n",
    "\n",
    "            \"hidden_layer_sizes\": [\n",
    "                trial.suggest_int(f\"layer1/3\", 600, 1200, step=200),\n",
    "                trial.suggest_int(f\"layer2/3\", 50, 400, step=50),\n",
    "                *([trial.suggest_int(f\"layer3/3\", 10, 50, step=10)])\n",
    "            ],\n",
    "            **common_params\n",
    "        }\n",
    "\n",
    "        trial.set_user_attr(\"model_params\", model_params)\n",
    "\n",
    "        model = MLPRegressor(**trial.user_attrs[\"model_params\"])\n",
    "        trial.set_user_attr(\"training_size\", 1.0)\n",
    "        trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "\n",
    "        try:\n",
    "            signal.alarm(max_duration_s)\n",
    "            model.fit(X_train_scaled, Y_train_pca)\n",
    "            signal.alarm(0)\n",
    "            Y_predict_pca = model.predict(X_test_scaled)\n",
    "            Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "            error = mean_squared_error(Y_test, Y_predict)\n",
    "        except TimeoutError:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        return error\n",
    "\n",
    "    study = optuna.create_study(study_name=f\"MLPRegressor_{learning_rate}_{max_duration_s}\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "    trials_due = 40 - len(study.trials)\n",
    "    if trials_due > 0:\n",
    "        previous_handler = signal.signal(signal.SIGALRM, keyboard_interrupt_handler)\n",
    "        study.optimize(objective, n_trials=trials_due)\n",
    "        signal.signal(signal.SIGALRM, previous_handler)\n",
    "\n",
    "    print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:08,618]\u001B[0m Using an existing study with name 'ElasticNet' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ElasticNet test dataset error: 0.06892972685134742 best_params: {'alpha': 5.98103619625795e-05, 'fit_intercept': True, 'l1_ratio': 0.5905390879114973, 'polynomial degree': 3}\n"
     ]
    }
   ],
   "source": [
    "# Linear regression with combined L1 and L2 priors as regularizer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "common_params = {\n",
    "    \"random_state\": 42,\n",
    "    \"tol\": 1e-5\n",
    "}\n",
    "\n",
    "MAX_POLYNOMIAL_DEGREE = 3\n",
    "\n",
    "def objective(trial):\n",
    "    global common_params\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (240, 360, 600, 1200)\n",
    "    \n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    max_iter = [20000, 40000, 80000, 160000][phase]\n",
    "    \n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\"model_params\", {\n",
    "        **common_params,\n",
    "        \"max_iter\": max_iter,\n",
    "        \"l1_ratio\": trial.suggest_float(\"l1_ratio\", 0.0, 1.0),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.00001, 1.0, log=True),\n",
    "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "    })\n",
    "    \n",
    "    trial.set_user_attr(\"polynomial degree\", trial.suggest_int(\"polynomial degree\", 1, MAX_POLYNOMIAL_DEGREE))\n",
    "    \n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(trial.user_attrs(\"polynomial degree\")),\n",
    "        ElasticNet(**trial.user_attrs(\"model_params\"))\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size,:], Y_train_pca[:training_size,:])\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        signal.alarm(0)\n",
    "    except TimeoutError:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "study = optuna.create_study(study_name=\"ElasticNet\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "trials_due = 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "    \n",
    "print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:08,809]\u001B[0m Using an existing study with name 'HistGradientBoostingRegressor' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: HistGradientBoostingRegressor test dataset error: 0.008577411300669628 best_params: {'l2_regularization': 0.001871946506788565, 'learning_rate': 0.1651977983535292, 'loss': 'squared_error', 'max_iter': 160, 'regularize': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    trial_params = {}\n",
    "    # poisson requires y > 0 which is not true in this case\n",
    "    trial_params[\"loss\"] = trial.suggest_categorical(\"loss\", [\"squared_error\", \"absolute_error\", \"quantile\"])\n",
    "    \n",
    "    if trial_params[\"loss\"] == \"quantile\":\n",
    "        trial_params[\"quantile\"] = trial.suggest_float(\"quantile\", 0, 1)\n",
    "    \n",
    "    trial_params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 0.00001, 0.5, log=True)\n",
    "    trial_params[\"max_iter\"] = trial.suggest_int(\"max_iter\", 20, 200, step=20)\n",
    "    if trial.suggest_categorical(\"regularize\", [True, False]):\n",
    "        trial_params[\"l2_regularization\"] = trial.suggest_float(\"l2_regularization\", 0.00001, 1.0, log=True)\n",
    "    \n",
    "    trial.set_user_attr(\"model_params\", {\n",
    "        \"random_state\": 42,\n",
    "        **trial_params\n",
    "    })\n",
    "    \n",
    "    model = MultiOutputRegressor(HistGradientBoostingRegressor(**trial.user_attrs(\"model_params\")))\n",
    "    \n",
    "    phase = min((trial.number // 30), 4)\n",
    "    \n",
    "    training_sizes = (0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (240, 360, 600, 1200)\n",
    "    \n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    \n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    \n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size,:], Y_train_pca[:training_size,:])\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        signal.alarm(0)\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "study = optuna.create_study(study_name=\"HistGradientBoostingRegressor\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "trials_due = 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:08,985]\u001B[0m Using an existing study with name 'BaggingRegressor' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: BaggingRegressor test dataset error: 0.012123721671364409 best_params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 10, 'max_iter': 50, 'max_samples': 320000, 'oob_score': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "    \n",
    "    training_sizes = (0.1, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (240, 360, 600, 1200)\n",
    "    \n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    \n",
    "    trial_params = {}\n",
    "    \n",
    "    trial_params[\"n_estimators\"] = trial.suggest_int(\"max_iter\", 10, 110, step=20)\n",
    "    trial_params[\"max_samples\"] = trial.suggest_int(\"max_samples\", 10000, (training_size // 10000) * 10000, step=10000)\n",
    "    trial_params[\"max_features\"] = trial.suggest_int(\"max_features\", 1, X_train_scaled.shape[1])\n",
    "    trial_params[\"bootstrap\"] = trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "    if trial_params[\"bootstrap\"]:\n",
    "        trial_params[\"oob_score\"] = trial.suggest_categorical(\"oob_score\", [True, False])\n",
    "    trial_params[\"bootstrap_features\"] = trial.suggest_categorical(\"bootstrap_features\", [True, False])\n",
    "    \n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\"model_params\", {\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "        **trial_params\n",
    "    })\n",
    "    \n",
    "    model = MultiOutputRegressor(BaggingRegressor(**trial.user_attrs(\"model_params\")))\n",
    "    \n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size,:], Y_train_pca[:training_size,:])\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        signal.alarm(0)\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "# try:\n",
    "#     optuna.delete_study(study_name=\"BaggingRegressor\", storage='sqlite:///../final/optuna.db')\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "study = optuna.create_study(study_name=\"BaggingRegressor\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "trials_due = 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "    \n",
    "print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:09,283]\u001B[0m Using an existing study with name 'RandomForrest' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: RandomForrest test dataset error: 0.0276166738609489 best_params: {'bootstrap': True, 'criterion': 'friedman_mse', 'max_features': 4, 'max_samples': 0.1753336890830068, 'min_samples_split': 2, 'n_estimators': 60}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.01, 0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (120, 180, 360, 600, 1200)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    \n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\"model_params\", {\n",
    "      \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200, step=10),\n",
    "      \"criterion\": trial.suggest_categorical(\"criterion\", [\"squared_error\", \"absolute_error\", \"friedman_mse\"]),\n",
    "      \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5),\n",
    "      \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True]),\n",
    "      \"max_samples\": trial.suggest_float(\"max_samples\", 0.0, 0.2),\n",
    "      \"max_features\": trial.suggest_int(\"max_features\", 1, X_train_scaled.shape[1] // 2),\n",
    "      \"n_jobs\": -1,\n",
    "      \"random_state\": 42\n",
    "    })\n",
    "    \n",
    "    model = RandomForestRegressor(**trial.user_attrs(\"model_params\"))\n",
    "    \n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size,:], Y_train_pca[:training_size,:])\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        signal.alarm(0)\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "study = optuna.create_study(study_name=\"RandomForrest\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "trials_due = 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:09,480]\u001B[0m Using an existing study with name 'ExtraTreesRegressor' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ExtraTreesRegressor test dataset error: 0.035638099610715714 best_params: {'bootstrap': True, 'criterion': 'friedman_mse', 'max_features': 5, 'max_samples': 0.18501919599037991, 'min_samples_split': 2, 'n_estimators': 182}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.01, 0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (120, 180, 360, 600, 1200)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    \n",
    "    trial.set_user_attr(\"model_params\", {\n",
    "      \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
    "      \"criterion\": trial.suggest_categorical(\"criterion\", [\"squared_error\", \"absolute_error\", \"friedman_mse\"]),\n",
    "      \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5),\n",
    "      \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True]),\n",
    "      \"max_samples\": trial.suggest_float(\"max_samples\", 0.0, 0.2),\n",
    "      \"max_features\": trial.suggest_int(\"max_features\", 1, X_train_scaled.shape[1] // 2),\n",
    "      \"n_jobs\": -1,\n",
    "      \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    model = ExtraTreesRegressor(**trial.user_attrs(\"model_params\"))\n",
    "    \n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size,:], Y_train_pca[:training_size,:])\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        signal.alarm(0)\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "study = optuna.create_study(study_name=\"ExtraTreesRegressor\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "trials_due = 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "    \n",
    "print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-09 12:11:09,677]\u001B[0m Using an existing study with name 'KNeighborsRegressor' instead of creating a new one.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KNeighborsRegressor test dataset error: 0.21196498486521023 best_params: {'algorithm': 'kd_tree', 'leaf_size': 13, 'n_neighbors': 9, 'p': 3, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "common_params = {\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.01, 0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (120, 180, 360, 600, 1200)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    \n",
    "    trial_params = {}\n",
    "    trial_params[\"n_neighbors\"] = trial.suggest_int(\"n_neighbors\", 1, 100)\n",
    "    trial_params[\"weights\"] = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "    trial_params[\"algorithm\"] = trial.suggest_categorical(\"algorithm\", [\"ball_tree\", \"kd_tree\", \"brute\"])\n",
    "    if trial_params[\"algorithm\"] != \"brute\":\n",
    "        trial_params[\"leaf_size\"] = trial.suggest_int(\"leaf_size\", 10, 50)\n",
    "    trial_params[\"p\"] = trial.suggest_int(\"p\", 1, 5)\n",
    "    \n",
    "    model_params = {\n",
    "        **trial_params,\n",
    "        **common_params\n",
    "    }\n",
    "    trial.set_user_attr(\"model_params\", model_params)\n",
    "    model = KNeighborsRegressor(**trial.user_attrs[\"model_params\"])\n",
    "    \n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size,:], Y_train_pca[:training_size,:])\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        signal.alarm(0)\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "study = optuna.create_study(study_name=\"KNeighborsRegressor\", storage='sqlite:///../final/optuna.db', load_if_exists=True)\n",
    "trials_due = 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "    \n",
    "print(f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
