{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751c4f4f",
   "metadata": {},
   "source": [
    "In this notebook we present comparison of [learning curves](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)) of best performing models trained in notebook `02. Training ML models`.\n",
    "\n",
    "This comparison justifies that using more data wont further improve the estimation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6450a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset\n",
      "transformed to orders of magnitude\n",
      "dropped treatment column\n",
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n",
      "train test split\n",
      "scaled\n",
      "applied pca\n"
     ]
    }
   ],
   "source": [
    "#loading dataset\n",
    "import numpy as np\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "dataset_size = inputs.shape[0]\n",
    "\n",
    "print(\"loaded dataset\")\n",
    "\n",
    "# transforming time profiles to its orders of magnitude\n",
    "\n",
    "LOWER_LIMIT = -9\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "print(\"transformed to orders of magnitude\")\n",
    "\n",
    "# dropping treatment column in input\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "\n",
    "print(\"dropped treatment column\")\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")\n",
    "\n",
    "print(\"train test split\")\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "with Path(f\"../final/scaler.pickle\").open(\"rb\") as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"scaled\")\n",
    "\n",
    "# applying principal component analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "PCA_COMPONENTS=12\n",
    "with Path(f\"../final/pca{PCA_COMPONENTS}.pickle\").open(\"rb\") as opened_file:\n",
    "    pca = pickle.load(opened_file)\n",
    "Y_train_pca = pca.transform(Y_train)\n",
    "\n",
    "print(\"applied pca\")\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47174ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, LinearRegression, LassoLars\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, BaggingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "for study_name, model_builder in [\n",
    "    (\"MLPRegressor_constant_600\", lambda t: MLPRegressor(**t.user_attrs[\"model_params\"]))\n",
    "    (\"LassoLars\", lambda t: None),\n",
    "    \"LinearRegression\",\n",
    "    \"Ridge\",\n",
    "    \"ElasticNet\",\n",
    "    \"HistGradientBoostingRegressor\",\n",
    "    \"BaggingRegressor\",\n",
    "    \"RandomForrest\",\n",
    "    \"ExtraTreesRegressor\",\n",
    "    \"KNeighborsRegressor\"\n",
    "]:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
