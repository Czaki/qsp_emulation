{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657032a4",
   "metadata": {},
   "source": [
    "In this notebook we have hand-adjusted the params of best performing ML model (Neural network implemenation `MLPRegressor`) selected from multiple models in notebook `02. Training ML models` to get even better performing one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9002c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset\n",
      "transformed to orders of magnitude\n",
      "dropped treatment column\n",
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n",
      "train test split\n",
      "scaled\n",
      "applied pca\n",
      "{'alpha': 0.0021976121677802214, 'learning_rate_init': 0.0025086281095413575, 'power_t': 0.5, 'hidden_layer_sizes': [800, 250, 20], 'tol': 5e-06, 'n_iter_no_change': 10, 'random_state': 42, 'warm_start': False, 'max_iter': 10000, 'batch_size': 2000, 'learning_rate': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "#loading dataset\n",
    "import numpy as np\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "dataset_size = inputs.shape[0]\n",
    "\n",
    "print(\"loaded dataset\")\n",
    "\n",
    "# transforming time profiles to its orders of magnitude\n",
    "\n",
    "LOWER_LIMIT = -9\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "print(\"transformed to orders of magnitude\")\n",
    "\n",
    "# dropping treatment column in input\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "\n",
    "print(\"dropped treatment column\")\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")\n",
    "\n",
    "print(\"train test split\")\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "with Path(f\"../final/scaler.pickle\").open(\"rb\") as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"scaled\")\n",
    "\n",
    "# applying principal component analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "PCA_COMPONENTS=12\n",
    "with Path(f\"../final/pca{PCA_COMPONENTS}.pickle\").open(\"rb\") as opened_file:\n",
    "    pca = pickle.load(opened_file)\n",
    "Y_train_pca = pca.transform(Y_train)\n",
    "\n",
    "print(\"applied pca\")\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "# Loading best performing optuna study \n",
    "study = optuna.load_study(study_name=f\"MLPRegressor_constant_600\", storage='sqlite:///../final/optuna.db')\n",
    "model_params = study.best_trial.user_attrs[\"model_params\"]\n",
    "print(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48414349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted 0 175.1\n",
      "error test: 0.002599849381178007, error train: 0.0026001043892114263 training_time: 195.80107188224792\n",
      "fitted 1 371.6\n",
      "error test: 0.00196335076492695, error train: 0.0019461638820849646 training_time: 388.2139699459076\n",
      "fitted 2 565.7\n",
      "error test: 0.0012887083779221224, error train: 0.0012774304893470778 training_time: 581.5762078762054\n",
      "fitted 3 757.8\n",
      "error test: 0.005476050076365092, error train: 0.005447488901730414 training_time: 772.6894669532776\n",
      "error test: 0.00042430121972095815, error train: 0.00041414605972777595 training_time: 965.0996699333191\n",
      "error test: 0.0002658747142208517, error train: 0.00026015462826451264 training_time: 1156.9848630428314\n",
      "error test: 0.00032315341588961085, error train: 0.00031774556200397957 training_time: 1346.0278718471527\n",
      "error test: 0.00017830213237130787, error train: 0.00017415880066484182 training_time: 1540.6659517288208\n",
      "error test: 0.00015665503752113576, error train: 0.00015141552560095424 training_time: 1729.6456530094147\n",
      "error test: 0.00014431642600299025, error train: 0.0001395976199735714 training_time: 1921.2602760791779\n",
      "error test: 0.00013086587542054224, error train: 0.00012670468836344383 training_time: 2112.5935909748077\n",
      "error test: 0.00011835949706445318, error train: 0.00011413976570787696 training_time: 2301.50252699852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "training_start = time.time()\n",
    "model = MLPRegressor(**{**model_params, \"batch_size\": 20000, \"learning_rate_init\": 0.01, \"alpha\": 0.00001})\n",
    "for i in range(4):\n",
    "    for j in range(10):\n",
    "        model.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    print(f\"fitted {i} {time.time() - training_start:.1f}\")\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")\n",
    "\n",
    "model2 = MLPRegressor(**{**model_params, \"batch_size\": 20000, \"learning_rate_init\": 0.003, \"alpha\": 0.00003})\n",
    "for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "    setattr(model2, variable_name, getattr(model, variable_name))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(10):\n",
    "        model2.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model2.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model2.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")    \n",
    "\n",
    "model3 = MLPRegressor(**{**model_params, \"batch_size\": 20000, \"learning_rate_init\": 0.001, \"alpha\": 0.0001})\n",
    "for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "    setattr(model3, variable_name, getattr(model2, variable_name))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(10):\n",
    "        model3.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model3.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model3.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")    \n",
    "    \n",
    "training_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5a62f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test: 0.00011486898482143657, error train: 0.00011041371032693849 training_time: 2531.1502380371094\n",
      "error test: 0.00010632929972035227, error train: 0.00010249740427607707 training_time: 2718.584650039673\n",
      "error test: 9.873049132474293e-05, error train: 9.486179975208429e-05 training_time: 2907.3779768943787\n",
      "error test: 9.978048183507067e-05, error train: 9.593812279995784e-05 training_time: 3096.79953289032\n",
      "error test: 0.00011802524270675386, error train: 0.00011403732743835556 training_time: 3284.9790318012238\n",
      "error test: 0.0001332832874652715, error train: 0.00013007423276541492 training_time: 3475.213138103485\n",
      "error test: 9.751010719360518e-05, error train: 9.328827910633931e-05 training_time: 3663.286122083664\n",
      "error test: 0.00010846891993241584, error train: 0.0001041237291668056 training_time: 3853.488116979599\n",
      "error test: 6.945236616957586e-05, error train: 6.55295224227693e-05 training_time: 4040.94211101532\n",
      "error test: 6.798756467593612e-05, error train: 6.396874392459019e-05 training_time: 4223.179073810577\n",
      "error test: 6.575795471758243e-05, error train: 6.188781695108829e-05 training_time: 4404.184032917023\n",
      "error test: 6.398481049708725e-05, error train: 6.014642476857574e-05 training_time: 4585.321450948715\n",
      "error test: 6.636144838053537e-05, error train: 6.237400549204041e-05 training_time: 4770.1979558467865\n",
      "error test: 6.485599243704493e-05, error train: 6.090593480321969e-05 training_time: 4955.734879732132\n",
      "error test: 6.271001120304195e-05, error train: 5.877720672188968e-05 training_time: 5148.261276006699\n",
      "error test: 5.791406824181016e-05, error train: 5.409535531279347e-05 training_time: 5339.123638868332\n"
     ]
    }
   ],
   "source": [
    "model4 = MLPRegressor(**{**model_params, \"batch_size\": 20000, \"learning_rate_init\": 0.001, \"alpha\": 0.0001})\n",
    "for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "    setattr(model4, variable_name, getattr(model3, variable_name))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(10):\n",
    "        model4.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model4.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model4.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")\n",
    "    \n",
    "    \n",
    "model5 = MLPRegressor(**{**model_params, \"batch_size\": 20000, \"learning_rate_init\": 0.0003, \"alpha\": 0.0001})\n",
    "for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "    setattr(model5, variable_name, getattr(model4, variable_name))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(10):\n",
    "        model5.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model5.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model5.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67c5fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test: 5.609778738136186e-05, error train: 5.23235884602929e-05 training_time: 5464.114265918732\n",
      "error test: 5.5997045707412863e-05, error train: 5.221308705723219e-05 training_time: 5587.8838810920715\n",
      "error test: 5.5912056454307255e-05, error train: 5.212551354245663e-05 training_time: 5713.504786014557\n",
      "error test: 5.5834242390853904e-05, error train: 5.2045404459587845e-05 training_time: 5838.271425962448\n",
      "error test: 5.574151744663473e-05, error train: 5.1945171524209724e-05 training_time: 5964.7364139556885\n",
      "error test: 5.56358640145835e-05, error train: 5.1838309050848234e-05 training_time: 6090.327034950256\n",
      "error test: 5.5523868366087516e-05, error train: 5.172095628331541e-05 training_time: 6213.642988920212\n",
      "error test: 5.5413936416687965e-05, error train: 5.161083918506648e-05 training_time: 6336.716297149658\n"
     ]
    }
   ],
   "source": [
    "model6 = MLPRegressor(**{**model_params, \"batch_size\": 50000, \"learning_rate_init\": 0.0001, \"alpha\": 0.0002})\n",
    "for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "    setattr(model6, variable_name, getattr(model5, variable_name))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(5):\n",
    "        model6.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model6.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model6.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f048e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test: 5.535181772134489e-05, error train: 5.155705752576035e-05 training_time: 6456.97175693512\n",
      "error test: 5.527926948704048e-05, error train: 5.148240955166337e-05 training_time: 6577.05282998085\n",
      "error test: 5.521603211735898e-05, error train: 5.141407538726785e-05 training_time: 6697.118595838547\n",
      "error test: 5.514383523121379e-05, error train: 5.134020572192409e-05 training_time: 6815.493922948837\n"
     ]
    }
   ],
   "source": [
    "model7 = MLPRegressor(**{**model_params, \"batch_size\": 50000, \"learning_rate_init\": 0.00003, \"alpha\": 0.0004})\n",
    "for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "    setattr(model7, variable_name, getattr(model6, variable_name))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        model7.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model7.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model7.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21d8eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test: 5.512047045051417e-05, error train: 5.1317610427962704e-05 training_time: 141.26645922660828\n",
      "error test: 5.514184664254254e-05, error train: 5.133288972155514e-05 training_time: 276.6796371936798\n",
      "error test: 5.5151747008367064e-05, error train: 5.1339569538849646e-05 training_time: 411.3547899723053\n",
      "error test: 5.511458140607042e-05, error train: 5.130226615030502e-05 training_time: 544.7778451442719\n"
     ]
    }
   ],
   "source": [
    "model8 = MLPRegressor(**{**model_params, \"batch_size\": 100000, \"learning_rate_init\": 0.00001, \"alpha\": 0.002})\n",
    "for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "    setattr(model8, variable_name, getattr(model7, variable_name))\n",
    "\n",
    "training_start = time.time()\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        model8.partial_fit(X_train_scaled, Y_train_pca)\n",
    "    error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model8.predict(X_test_scaled)))\n",
    "    error_train = mean_squared_error(Y_train, pca.inverse_transform(model8.predict(X_train_scaled)))\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c99a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(f\"../final/MLPRegressor.pickle\").open(\"wb\") as opened_file:\n",
    "    pickle.dump(model8, opened_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb70d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
