{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657032a4",
   "metadata": {},
   "source": [
    "In this notebook we have hand-adjusted the params of best performing ML model (Neural network implemenation `MLPRegressor`) selected from multiple models in notebook `02. Training ML models` to get even better performing one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9002c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset\n",
      "transformed to orders of magnitude\n",
      "dropped treatment column\n",
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n",
      "train test split\n",
      "scaled\n",
      "applied pca with 12 components. Unexplained variance ratio: 1.5039885656489999e-06\n"
     ]
    }
   ],
   "source": [
    "LOWER_LIMIT = -9\n",
    "PCA_COMPONENTS=12\n",
    "\n",
    "#loading dataset\n",
    "import numpy as np\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "dataset_size = inputs.shape[0]\n",
    "\n",
    "print(\"loaded dataset\")\n",
    "\n",
    "# transforming time profiles to its orders of magnitude\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "print(\"transformed to orders of magnitude\")\n",
    "\n",
    "# dropping treatment column in input\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "\n",
    "print(\"dropped treatment column\")\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")\n",
    "\n",
    "print(\"train test split\")\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "scaler_path = Path(f\"../final/scaler.pickle\")\n",
    "scaler = None\n",
    "if scaler_path.exists():\n",
    "    with scaler_path.open(\"rb\") as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "else:\n",
    "    scaler = CustomScaler().fit(X_train)\n",
    "    with scaler_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(scaler, opened_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"scaled\")\n",
    "\n",
    "# applying principal component analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_path = Path(f\"../final/pca{PCA_COMPONENTS}_{LOWER_LIMIT}.pickle\")\n",
    "\n",
    "if pca_path.exists():\n",
    "    with pca_path.open(\"rb\") as opened_file:\n",
    "        pca = pickle.load(opened_file)\n",
    "    Y_train_pca = pca.transform(Y_train)\n",
    "else: \n",
    "    pca = PCA(n_components=PCA_COMPONENTS)\n",
    "    Y_train_pca = pca.fit_transform(Y_train)\n",
    "    with pca_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(pca, opened_file)\n",
    "\n",
    "from functools import reduce\n",
    "print(f\"applied pca with {PCA_COMPONENTS} components. Unexplained variance ratio: {reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0)}\")\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a14374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previous ../final/MLPRegressor_600_100_40_12_-9_0.pickle\n",
      "Iteration 167, loss = 0.00144282\n",
      "Iteration 168, loss = 0.00136837\n",
      "Iteration 169, loss = 0.00137553\n",
      "Iteration 170, loss = 0.00138254\n",
      "Iteration 171, loss = 0.00137603\n",
      "Iteration 172, loss = 0.00139078\n",
      "Iteration 173, loss = 0.00137606\n",
      "Iteration 174, loss = 0.00138355\n",
      "Training loss did not improve more than tol=0.000005 for 5 consecutive epochs. Stopping.\n",
      "error test: 6.028506204801249e-05, error train: 5.5534500142889405e-05 training_time: 379.1\n",
      "saving ../final/MLPRegressor_600_100_40_12_-9_1.pickle\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_1.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00200026580476465,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 8.399372157828117e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error\": 6.028506204801249e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error\": 5.5534500142889405e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n",
      "Iteration 175, loss = 0.00094254\n",
      "Iteration 176, loss = 0.00089293\n",
      "Iteration 177, loss = 0.00089183\n",
      "Iteration 178, loss = 0.00089426\n",
      "Iteration 179, loss = 0.00089754\n",
      "Iteration 180, loss = 0.00090171\n",
      "Iteration 181, loss = 0.00089563\n",
      "Iteration 182, loss = 0.00089667\n",
      "Training loss did not improve more than tol=0.000003 for 5 consecutive epochs. Stopping.\n",
      "error test: 6.079443740282251e-05, error train: 5.611050223680676e-05 training_time: 786.3\n",
      "saving ../final/MLPRegressor_600_100_40_12_-9_2.pickle\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_2.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.001000132902382325,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 2.5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 4.1996860789140585e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 2.5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error\": 6.079443740282251e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error\": 5.611050223680676e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n",
      "Iteration 183, loss = 0.00066123\n",
      "Iteration 184, loss = 0.00065212\n",
      "Iteration 185, loss = 0.00065171\n",
      "Iteration 186, loss = 0.00065349\n",
      "Iteration 187, loss = 0.00065103\n",
      "Iteration 188, loss = 0.00065138\n",
      "Iteration 189, loss = 0.00065259\n",
      "Iteration 190, loss = 0.00064988\n",
      "Training loss did not improve more than tol=0.000001 for 5 consecutive epochs. Stopping.\n",
      "error test: 5.509884918280646e-05, error train: 5.051570722367543e-05 training_time: 1198.4\n",
      "saving ../final/MLPRegressor_600_100_40_12_-9_3.pickle\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_3.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.0005000664511911625,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 1.25e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 2.0998430394570292e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 1.25e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error\": 5.509884918280646e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error\": 5.051570722367543e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n",
      "Iteration 191, loss = 0.00053063\n",
      "Iteration 192, loss = 0.00052771\n",
      "Iteration 193, loss = 0.00052853\n",
      "Iteration 194, loss = 0.00052726\n",
      "Iteration 195, loss = 0.00052737\n",
      "Iteration 196, loss = 0.00052715\n",
      "Iteration 197, loss = 0.00052656\n",
      "Iteration 198, loss = 0.00052631\n",
      "Training loss did not improve more than tol=0.000001 for 5 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from threadpoolctl import threadpool_limits\n",
    "from cpuinfo import get_cpu_info\n",
    "import json\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [600, 100, 40]\n",
    "training_start = time.time()\n",
    "for k in range(5):\n",
    "    last_file = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.pickle\"\n",
    "    info_filename = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "    \n",
    "    if Path(last_file).exists():\n",
    "        print(f\"loading previous {last_file}\")\n",
    "        with Path(last_file).open(\"rb\") as opened_file:\n",
    "            model = pickle.load(opened_file)\n",
    "        with Path(info_filename).open('r') as opened_file:\n",
    "            print(opened_file.read())\n",
    "        continue\n",
    "    \n",
    "    if k > 0:\n",
    "        old_model = model\n",
    "    model_params = {\n",
    "        \"alpha\": 0.0040005316095293 / (2 ** k),\n",
    "        \"batch_size\": 2000,\n",
    "        \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "        \"learning_rate\": \"constant\",\n",
    "        \"learning_rate_init\": 0.00016798744315656234 / (2 ** k),\n",
    "        \"max_iter\": 300,\n",
    "        \"n_iter_no_change\": 5,\n",
    "        \"random_state\": 42,\n",
    "        \"tol\": 1e-05 / (2**k),\n",
    "        \"epsilon\": 1e-08 / (2**k),\n",
    "        \"verbose\": True,\n",
    "        \"warm_start\": k > 0\n",
    "    }\n",
    "    model = MLPRegressor(**model_params)\n",
    "    if k > 0:\n",
    "        for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "            setattr(model, variable_name, getattr(old_model, variable_name))\n",
    "        \n",
    "    with threadpool_limits(limits=get_cpu_info()[\"count\"], user_api='blas'):\n",
    "        model.fit(X_train_scaled, Y_train_pca)\n",
    "        error_test  = mean_squared_error(Y_test,  pca.inverse_transform(model.predict(X_test_scaled)))\n",
    "        error_train = mean_squared_error(Y_train, pca.inverse_transform(model.predict(X_train_scaled)))\n",
    "\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start:.1f}\")\n",
    "        \n",
    "    with Path(last_file).open(\"wb\") as opened_file:\n",
    "        print(f\"saving {last_file}\")\n",
    "        pickle.dump(model, opened_file)\n",
    "    info_filename = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "    with Path(info_filename).open('w') as opened_file:\n",
    "        info = json.dumps({\n",
    "            \"cpu_info\": {key: get_cpu_info()[key] for key in [\"arch\", \"bits\", \"brand_raw\", \"count\", \"l2_cache_size\"]},\n",
    "            \"pca_components\": PCA_COMPONENTS,\n",
    "            \"pca_unexplained_variance_ratio\": reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0),\n",
    "            \"tumour_lower_size_limit_l\": 10 ** LOWER_LIMIT,\n",
    "            \"tumour_lower_size_limit_log10_l\": LOWER_LIMIT,\n",
    "            \"model_params\": model_params,\n",
    "            \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
    "            \"test_error\": error_test,\n",
    "            \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
    "            \"train_error\": error_train\n",
    "        }, sort_keys=True, indent=4)\n",
    "        print(f\"saving info to file: {info_filename} {info}\")\n",
    "        opened_file.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb70d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
