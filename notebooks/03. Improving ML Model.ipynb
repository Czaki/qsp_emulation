{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657032a4",
   "metadata": {},
   "source": [
    "In this notebook we have hand-adjusted the params of best performing ML model (Neural network implemenation `MLPRegressor`) selected from multiple models in notebook `02. Training ML models` to get even better performing one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9002c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset\n",
      "transformed to orders of magnitude\n",
      "dropped treatment column\n",
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n",
      "train test split\n",
      "scaled\n",
      "applied pca with 12 components. Unexplained variance ratio: 1.5039885656489999e-06\n"
     ]
    }
   ],
   "source": [
    "LOWER_LIMIT = -9\n",
    "PCA_COMPONENTS=12\n",
    "\n",
    "#loading dataset\n",
    "import numpy as np\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "dataset_size = inputs.shape[0]\n",
    "\n",
    "print(\"loaded dataset\")\n",
    "\n",
    "# transforming time profiles to its orders of magnitude\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "def apply_size_limit(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    x[x < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return x\n",
    "\n",
    "def apply_absolute_size_limit(outputs: np.array) -> np.array:\n",
    "    limit = 10 ** LOWER_LIMIT\n",
    "    x = np.copy(outputs)\n",
    "    x[x < limit] = 0\n",
    "    return x\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "print(\"transformed to orders of magnitude\")\n",
    "\n",
    "# dropping treatment column in input\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "\n",
    "print(\"dropped treatment column\")\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "Y_train_absolute = apply_absolute_size_limit(outputs[:train_size, :])\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "Y_test_absolute = apply_absolute_size_limit(outputs[train_size:(train_size + test_size), :])\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")\n",
    "print(\"train test split\")\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "scaler_path = Path(f\"../final/scaler.pickle\")\n",
    "scaler = None\n",
    "if scaler_path.exists():\n",
    "    with scaler_path.open(\"rb\") as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "else:\n",
    "    scaler = CustomScaler().fit(X_train)\n",
    "    with scaler_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(scaler, opened_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"scaled\")\n",
    "\n",
    "# applying principal component analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_path = Path(f\"../final/pca{PCA_COMPONENTS}_{LOWER_LIMIT}.pickle\")\n",
    "\n",
    "if pca_path.exists():\n",
    "    with pca_path.open(\"rb\") as opened_file:\n",
    "        pca = pickle.load(opened_file)\n",
    "    Y_train_pca = pca.transform(Y_train)\n",
    "else: \n",
    "    pca = PCA(n_components=PCA_COMPONENTS)\n",
    "    Y_train_pca = pca.fit_transform(Y_train)\n",
    "    with pca_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(pca, opened_file)\n",
    "\n",
    "from functools import reduce\n",
    "print(f\"applied pca with {PCA_COMPONENTS} components. Unexplained variance ratio: {reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0)}\")\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a14374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previous ../final/MLPRegressor_600_100_40_12_-9_0.pickle\n",
      "error test: 5.6158462752489024e-05, error train: 5.167492621982182e-05 training_time: 118.3\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_0.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.0040005316095293,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 1e-08,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 0.00016798744315656234,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 1e-05,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": false\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.553999355170883e-08,\n",
      "    \"test_error_orders_of_magnitude\": 5.6158462752489024e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.6111250474356296e-08,\n",
      "    \"train_error_orders_of_magnitude\": 5.167492621982182e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-9_1.pickle\n",
      "error test: 5.7259925022725195e-05, error train: 5.2692317652654685e-05 training_time: 256.4\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_1.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00200026580476465,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 8.399372157828117e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.433554246371471e-08,\n",
      "    \"test_error_orders_of_magnitude\": 5.7259925022725195e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.5183099569593443e-08,\n",
      "    \"train_error_orders_of_magnitude\": 5.2692317652654685e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-9_2.pickle\n",
      "error test: 5.778383797412412e-05, error train: 5.3280704888659864e-05 training_time: 394.6\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_2.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.001000132902382325,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 2.5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 4.1996860789140585e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 2.5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.4141147334981617e-08,\n",
      "    \"test_error_orders_of_magnitude\": 5.778383797412412e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.478518913056308e-08,\n",
      "    \"train_error_orders_of_magnitude\": 5.3280704888659864e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-9_3.pickle\n",
      "error test: 5.1817463314287125e-05, error train: 4.7420734210613026e-05 training_time: 538.8\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_3.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.0005000664511911625,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 1.25e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 2.0998430394570292e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 1.25e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.3839850243592668e-08,\n",
      "    \"test_error_orders_of_magnitude\": 5.1817463314287125e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.4039597632026395e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.7420734210613026e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-9_4.pickle\n",
      "error test: 5.096887804159093e-05, error train: 4.66700791443873e-05 training_time: 659.7\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-9_4.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00025003322559558126,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 6.25e-10,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 1.0499215197285146e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 6.25e-07,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 1.5039885656489999e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.3724014812159053e-08,\n",
      "    \"test_error_orders_of_magnitude\": 5.096887804159093e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.389052376275253e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.66700791443873e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-09,\n",
      "    \"tumour_lower_size_limit_log10_l\": -9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from threadpoolctl import threadpool_limits\n",
    "from cpuinfo import get_cpu_info\n",
    "import json\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [600, 100, 40]\n",
    "training_start = time.time()\n",
    "for k in range(5):\n",
    "    last_file = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.pickle\"\n",
    "    info_filename = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "    \n",
    "    if Path(last_file).exists():\n",
    "        print(f\"loading previous {last_file}\")\n",
    "        with Path(last_file).open(\"rb\") as opened_file:\n",
    "            model = pickle.load(opened_file)\n",
    "#         with Path(info_filename).open('r') as opened_file:\n",
    "#             print(opened_file.read())\n",
    "#         continue\n",
    "    \n",
    "    if k > 0:\n",
    "        old_model = model\n",
    "    model_params = {\n",
    "        \"alpha\": 0.0040005316095293 / (2 ** k),\n",
    "        \"batch_size\": 2000,\n",
    "        \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "        \"learning_rate\": \"constant\",\n",
    "        \"learning_rate_init\": 0.00016798744315656234 / (2 ** k),\n",
    "        \"max_iter\": 400,\n",
    "        \"n_iter_no_change\": 5,\n",
    "        \"random_state\": 42,\n",
    "        \"tol\": 1e-05 / (2**k),\n",
    "        \"epsilon\": 1e-08 / (2**k),\n",
    "        \"verbose\": True,\n",
    "        \"warm_start\": k > 0\n",
    "    }\n",
    "#     model = MLPRegressor(**model_params)\n",
    "#     if k > 0:\n",
    "#         for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "#             setattr(model, variable_name, getattr(old_model, variable_name))\n",
    "        \n",
    "    with threadpool_limits(limits=get_cpu_info()[\"count\"], user_api='blas'):\n",
    "#         model.fit(X_train_scaled, Y_train_pca)\n",
    "        error_test  = mean_squared_error(Y_test,  apply_size_limit(pca.inverse_transform(model.predict(X_test_scaled))))\n",
    "        error_train = mean_squared_error(Y_train, apply_size_limit(pca.inverse_transform(model.predict(X_train_scaled))))\n",
    "        error_test_absolute  = mean_squared_error(Y_test_absolute,  output_untransform(pca.inverse_transform(model.predict(X_test_scaled))))\n",
    "        error_train_absolute = mean_squared_error(Y_train_absolute, output_untransform(pca.inverse_transform(model.predict(X_train_scaled))))\n",
    "\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start:.1f}\")\n",
    "        \n",
    "#     with Path(last_file).open(\"wb\") as opened_file:\n",
    "#         print(f\"saving {last_file}\")\n",
    "#         pickle.dump(model, opened_file)\n",
    "    with Path(info_filename).open('w') as opened_file:\n",
    "        info = json.dumps({\n",
    "            \"cpu_info\": {key: get_cpu_info()[key] for key in [\"arch\", \"bits\", \"brand_raw\", \"count\", \"l2_cache_size\"]},\n",
    "            \"pca_components\": PCA_COMPONENTS,\n",
    "            \"pca_unexplained_variance_ratio\": reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0),\n",
    "            \"tumour_lower_size_limit_l\": 10 ** LOWER_LIMIT,\n",
    "            \"tumour_lower_size_limit_log10_l\": LOWER_LIMIT,\n",
    "            \"model_params\": model_params,\n",
    "            \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
    "            \"test_error_orders_of_magnitude\": error_test,\n",
    "            \"test_error_absolute\": error_test_absolute,\n",
    "            \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
    "            \"train_error_orders_of_magnitude\": error_train,\n",
    "            \"train_error_absolute\": error_train_absolute\n",
    "        }, sort_keys=True, indent=4)\n",
    "        print(f\"saving info to file: {info_filename} {info}\")\n",
    "        opened_file.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb70d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
