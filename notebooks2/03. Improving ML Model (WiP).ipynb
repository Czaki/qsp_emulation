{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69807378",
   "metadata": {},
   "source": [
    "In this notebook we have hand-adjusted the params of best performing ML model (Neural network implemenation `MLPRegressor`) selected from multiple models in notebook `02. Training ML models` to get even better performing one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcd0e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n",
      "{'alpha': 0.0021976121677802214, 'learning_rate_init': 0.0025086281095413575, 'power_t': 0.5, 'hidden_layer_sizes': [800, 250, 20], 'tol': 5e-06, 'n_iter_no_change': 10, 'random_state': 42, 'warm_start': False, 'max_iter': 10000, 'batch_size': 2000, 'learning_rate': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "#loading dataset\n",
    "import numpy as np\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "dataset_size = inputs.shape[0]\n",
    "\n",
    "# transforming time profiles to its orders of magnitude\n",
    "\n",
    "LOWER_LIMIT = -9\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "\n",
    "\n",
    "# dropping treatment column in input\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")\n",
    "\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "with Path(f\"../final/scaler.pickle\").open(\"rb\") as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# applying principal component analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "PCA_COMPONENTS=12\n",
    "with Path(f\"../final/pca{PCA_COMPONENTS}.pickle\").open(\"rb\") as opened_file:\n",
    "    pca = pickle.load(opened_file)\n",
    "Y_train_pca = pca.transform(Y_train)\n",
    "\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "# Loading best performing optuna study \n",
    "study = optuna.load_study(study_name=f\"MLPRegressor_constant_600\", storage='sqlite:///../final/optuna.db')\n",
    "model_params = study.best_trial.user_attrs[\"model_params\"]\n",
    "print(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b314f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Training model without 10 minute training limit\n",
    "training_start = time.time()\n",
    "model = MLPRegressor(**model_params)\n",
    "model.fit(X_train_scaled, Y_train_pca)\n",
    "training_end = time.time()\n",
    "Y_predict_pca = model.predict(X_test_scaled)\n",
    "Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "error = mean_squared_error(Y_test, Y_predict)\n",
    "\n",
    "print(f\"error: {error}, training_time: {training_end - training_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start2 = time.time()\n",
    "model2 = MLPRegressor(**model_params, tol=1e-6, batch_size=4000, warm_start=True, learning_rate_init=0.0005, max_iter=100)\n",
    "model2.fit(X_train_scaled, Y_train_pca)\n",
    "training_end2 = time.time()\n",
    "Y_predict_pca = model.predict(X_test_scaled)\n",
    "Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "error2 = mean_squared_error(Y_test, Y_predict)\n",
    "print(f\"error: {error2}, training_time: {training_end2 - training_start2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
